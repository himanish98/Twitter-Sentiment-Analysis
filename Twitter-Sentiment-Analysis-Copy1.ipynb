{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"x_y_train.csv\")\n",
    "test_data = pd.read_csv(\"x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data\n",
    "x_train_data = train_data[\"text\"]\n",
    "y_train_data = train_data[\"airline_sentiment\"]\n",
    "\n",
    "## Testing data\n",
    "x_test_data = test_data[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising Words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data\n",
    "x_train = []\n",
    "for i in range(len(x_train_data)):\n",
    "    x_train.append(word_tokenize(x_train_data[i].lower()))\n",
    "    \n",
    "## Testing data\n",
    "x_test = []\n",
    "for i in range(len(x_test_data)):\n",
    "    x_test.append(word_tokenize(x_test_data[i].lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Removing twitter-handles, stop-words, punctuations and taking care of uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the twitter handles\n",
    "for x in x_train:\n",
    "    handles = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == str('@'):\n",
    "            handles.append(x[i])\n",
    "            handles.append(x[i+1])\n",
    "#     print(handles)  \n",
    "    for p in handles:\n",
    "        x.remove(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "## Importing punctuations\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "## Adding Punctuations to our stop words\n",
    "stop += punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "## Function to change the pos_tag to simpler values which can be passed to lemmatize\n",
    "\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Lemmatizer\n",
    "from  nltk  import  pos_tag\n",
    "from  nltk.stem  import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('appple', 'NN')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag([\"appple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to clean the words by removing stopwords, taking care of cases and lemmatizing the word to give root words only\n",
    "def cleaned(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_doc = [(cleaned(x_train[i]), y_train[i] ) for i in range(len(x_train))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_train, x_train_test, y_train_train, y_train_test = train_test_split(x_train_data, y_train_data, random_state = 0, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(max_features=100, ngram_range=(1,2), stop_words=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fit transform the training data\n",
    "train_features = count_vec.fit_transform(x_train_train)\n",
    "\n",
    "## Only transform the testing data according to the features which was fit using x_train\n",
    "test_features = count_vec.transform(x_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent',\n",
       " 'airline',\n",
       " 'airport',\n",
       " 'americanair',\n",
       " 'amp',\n",
       " 'another',\n",
       " 'back',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'bags',\n",
       " 'call',\n",
       " 'cancelled',\n",
       " 'cancelled flighted',\n",
       " 'cancelled flightled',\n",
       " 'change',\n",
       " 'check',\n",
       " 'co',\n",
       " 'could',\n",
       " 'customer',\n",
       " 'customer service',\n",
       " 'day',\n",
       " 'delay',\n",
       " 'delayed',\n",
       " 'dm',\n",
       " 'due',\n",
       " 'email',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'first',\n",
       " 'flight',\n",
       " 'flighted',\n",
       " 'flightled',\n",
       " 'flights',\n",
       " 'fly',\n",
       " 'flying',\n",
       " 'gate',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'go',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'guys',\n",
       " 'help',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'http',\n",
       " 'http co',\n",
       " 'jetblue',\n",
       " 'know',\n",
       " 'last',\n",
       " 'late',\n",
       " 'late flight',\n",
       " 'like',\n",
       " 'lost',\n",
       " 'love',\n",
       " 'luggage',\n",
       " 'make',\n",
       " 'minutes',\n",
       " 'much',\n",
       " 'need',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'one',\n",
       " 'people',\n",
       " 'phone',\n",
       " 'plane',\n",
       " 'please',\n",
       " 'really',\n",
       " 'seat',\n",
       " 'service',\n",
       " 'someone',\n",
       " 'southwestair',\n",
       " 'still',\n",
       " 'take',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'ticket',\n",
       " 'time',\n",
       " 'today',\n",
       " 'told',\n",
       " 'tomorrow',\n",
       " 'travel',\n",
       " 'trying',\n",
       " 'united',\n",
       " 'us',\n",
       " 'usairways',\n",
       " 'virginamerica',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'want',\n",
       " 'way',\n",
       " 'weather',\n",
       " 'worst',\n",
       " 'would',\n",
       " 'yes']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying SVC\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(train_features, y_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69071038251366124"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(test_features, y_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_features, y_train_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68160291438979959"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(test_features, y_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bayes = MultinomialNB()\n",
    "bayes.fit(train_features, y_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70236794171220396"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.score(test_features, y_train_test)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
